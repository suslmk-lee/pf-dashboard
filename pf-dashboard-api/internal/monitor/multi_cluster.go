package monitor

import (
	"context"
	"fmt"
	"log"
	"os"
	"strings"
	"sync"
	"time"

	"github.com/minkyulee/pf-dashboard-backend/internal/eventlog"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/clientcmd"
)

const (
	// Member í´ëŸ¬ìŠ¤í„° Context ì´ë¦„
	Member1ContextName = "karmada-member1-ctx"
	Member2ContextName = "karmada-member2-ctx"
)

// MultiClusterMonitor ë©€í‹° í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§
type MultiClusterMonitor struct {
	clusterMonitor *ClusterMonitor
	trafficMonitor *TrafficMonitor
	eventLog       *eventlog.EventLog
	memberClusters map[string]*kubernetes.Clientset
	watchers       []chan []ClusterInfo
	mu             sync.RWMutex
	lastStatus     map[string]string            // ì´ì „ í´ëŸ¬ìŠ¤í„° ìƒíƒœ ì¶”ì 
	lastNodeStatus map[string]map[string]string // ì´ì „ ë…¸ë“œ ìƒíƒœ ì¶”ì  [clusterID][nodeName]status
}

// NewMultiClusterMonitor ìƒˆ ë©€í‹° í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„° ìƒì„±
func NewMultiClusterMonitor(eventLog *eventlog.EventLog) *MultiClusterMonitor {
	mcm := &MultiClusterMonitor{
		eventLog:       eventLog,
		memberClusters: make(map[string]*kubernetes.Clientset),
		lastStatus:     make(map[string]string),
		lastNodeStatus: make(map[string]map[string]string),
	}

	// Member í´ëŸ¬ìŠ¤í„° í´ë¼ì´ì–¸íŠ¸ ìƒì„±
	mcm.initMemberClusters()

	// ê¸°ë³¸ ClusterMonitor ìƒì„±
	mcm.clusterMonitor = NewClusterMonitor(eventLog)

	// TrafficMonitor ìƒì„±
	mcm.trafficMonitor = NewTrafficMonitor(mcm.memberClusters)

	return mcm
}

// initMemberClusters Member í´ëŸ¬ìŠ¤í„° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
func (mcm *MultiClusterMonitor) initMemberClusters() {
	kubeconfig := os.Getenv("KUBECONFIG")
	if kubeconfig == "" {
		kubeconfig = os.Getenv("HOME") + "/.kube/config"
	}

	// kubeconfig ë¡œë“œ
	config, err := clientcmd.LoadFromFile(kubeconfig)
	if err != nil {
		log.Printf("Failed to load kubeconfig: %v", err)
		return
	}

	// member-cluster1, member-cluster2 context ì°¾ê¸°
	memberContexts := []string{Member1ContextName, Member2ContextName}

	for _, contextName := range memberContexts {
		if _, exists := config.Contexts[contextName]; !exists {
			log.Printf("Context %s not found in kubeconfig", contextName)
			continue
		}

		// Contextë³„ë¡œ clientset ìƒì„±
		clientConfig := clientcmd.NewNonInteractiveDeferredLoadingClientConfig(
			&clientcmd.ClientConfigLoadingRules{ExplicitPath: kubeconfig},
			&clientcmd.ConfigOverrides{CurrentContext: contextName},
		)

		restConfig, err := clientConfig.ClientConfig()
		if err != nil {
			log.Printf("Failed to create config for %s: %v", contextName, err)
			continue
		}

		clientset, err := kubernetes.NewForConfig(restConfig)
		if err != nil {
			log.Printf("Failed to create clientset for %s: %v", contextName, err)
			continue
		}

		mcm.memberClusters[contextName] = clientset
		log.Printf("Successfully connected to %s", contextName)
	}
}

// CheckClusters ì‹¤ì œ í´ëŸ¬ìŠ¤í„° ìƒíƒœ ì²´í¬
func (mcm *MultiClusterMonitor) CheckClusters() []ClusterInfo {
	clusters := make([]ClusterInfo, 0, 2)

	// ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„¤ì •
	namespace := os.Getenv("APP_NAMESPACE")
	if namespace == "" {
		namespace = "default"
	}

	// Member Cluster 1
	member1Info := mcm.getClusterInfo(Member1ContextName, "member1", "Member1 Cluster", namespace)
	clusters = append(clusters, member1Info)
	mcm.checkNodeStatusChanges("member1", member1Info.Name, member1Info.Nodes)
	mcm.checkStatusChange("member1", member1Info.Name, member1Info.Status, member1Info.Nodes)

	// Member Cluster 2
	member2Info := mcm.getClusterInfo(Member2ContextName, "member2", "Member2 Cluster", namespace)
	clusters = append(clusters, member2Info)
	mcm.checkNodeStatusChanges("member2", member2Info.Name, member2Info.Nodes)
	mcm.checkStatusChange("member2", member2Info.Name, member2Info.Status, member2Info.Nodes)

	return clusters
}

// checkStatusChange í´ëŸ¬ìŠ¤í„° ìƒíƒœ ë³€í™” ê°ì§€ ë° ì´ë²¤íŠ¸ ìƒì„±
func (mcm *MultiClusterMonitor) checkStatusChange(clusterID, clusterName, currentStatus string, nodes []NodeInfo) {
	mcm.mu.Lock()
	defer mcm.mu.Unlock()

	lastStatus, exists := mcm.lastStatus[clusterID]
	
	// ìƒíƒœ ë³€í™”ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì´ë²¤íŠ¸ ìƒì„±
	if exists && lastStatus != currentStatus {
		var eventType, message string
		
		if currentStatus == "failure" {
			eventType = "critical"
			message = fmt.Sprintf("ğŸ”´ %s is DOWN - No ready nodes available", clusterName)
			log.Printf("[ALERT] %s", message)
		} else if currentStatus == "ready" && lastStatus == "failure" {
			// Ready ë…¸ë“œ ê°œìˆ˜ ê³„ì‚°
			readyCount := 0
			for _, node := range nodes {
				if node.Status == "Ready" {
					readyCount++
				}
			}
			
			eventType = "success"
			if readyCount == len(nodes) {
				message = fmt.Sprintf("âœ… %s RECOVERED - All %d nodes are ready", clusterName, len(nodes))
			} else {
				message = fmt.Sprintf("âœ… %s RECOVERED - %d/%d nodes are ready", clusterName, readyCount, len(nodes))
			}
			log.Printf("[INFO] %s", message)
		}
		
		if message != "" {
			mcm.eventLog.AddEvent(eventType, message)
		}
	}
	
	// í˜„ì¬ ìƒíƒœ ì €ì¥
	mcm.lastStatus[clusterID] = currentStatus
}

// checkNodeStatusChanges ë…¸ë“œë³„ ìƒíƒœ ë³€í™” ê°ì§€ ë° ì´ë²¤íŠ¸ ìƒì„±
func (mcm *MultiClusterMonitor) checkNodeStatusChanges(clusterID, clusterName string, nodes []NodeInfo) {
	mcm.mu.Lock()
	defer mcm.mu.Unlock()

	// í´ëŸ¬ìŠ¤í„°ë³„ ë…¸ë“œ ìƒíƒœ ë§µì´ ì—†ìœ¼ë©´ ìƒì„±
	if mcm.lastNodeStatus[clusterID] == nil {
		mcm.lastNodeStatus[clusterID] = make(map[string]string)
	}

	// ê° ë…¸ë“œì˜ ìƒíƒœ ë³€í™” í™•ì¸
	for _, node := range nodes {
		lastStatus, exists := mcm.lastNodeStatus[clusterID][node.Name]
		currentStatus := node.Status

		// ìƒíƒœ ë³€í™”ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì´ë²¤íŠ¸ ìƒì„±
		if exists && lastStatus != currentStatus {
			var eventType, message string

			if currentStatus == "Ready" && lastStatus != "Ready" {
				eventType = "success"
				message = fmt.Sprintf("âœ… Node %s in %s is now READY", node.Name, clusterName)
				log.Printf("[INFO] %s", message)
			} else if currentStatus != "Ready" && lastStatus == "Ready" {
				eventType = "critical"
				message = fmt.Sprintf("ğŸ”´ Node %s in %s is now NOT READY", node.Name, clusterName)
				log.Printf("[ALERT] %s", message)
			}

			if message != "" {
				mcm.eventLog.AddEvent(eventType, message)
			}
		}

		// í˜„ì¬ ìƒíƒœ ì €ì¥
		mcm.lastNodeStatus[clusterID][node.Name] = currentStatus
	}
}

// getClusterInfo íŠ¹ì • í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¡°íšŒ
func (mcm *MultiClusterMonitor) getClusterInfo(contextName, id, name, namespace string) ClusterInfo {
	info := ClusterInfo{
		ID:       id,
		Name:     name,
		Status:   "ready",
		Pods:     0,
		Region:   "Seoul",
		Sessions: 0,
		Nodes:    []NodeInfo{},
		PodList:  []PodInfo{},
	}

	clientset, exists := mcm.memberClusters[contextName]
	if !exists {
		log.Printf("Clientset for %s not found", contextName)
		info.Status = "failure"
		return info
	}

	ctx := context.Background()

	// ë…¸ë“œ ì •ë³´ ìˆ˜ì§‘
	nodes, err := clientset.CoreV1().Nodes().List(ctx, metav1.ListOptions{})
	if err != nil {
		log.Printf("Failed to list nodes in %s: %v", contextName, err)
		info.Status = "failure"
		return info
	}

	// ìµœì†Œ 1ê°œ ì´ìƒì˜ ë…¸ë“œê°€ Ready ìƒíƒœì¸ì§€ í™•ì¸
	readyNodeCount := 0
	for _, node := range nodes.Items {
		nodeInfo := mcm.extractNodeInfo(&node)
		info.Nodes = append(info.Nodes, nodeInfo)

		if nodeInfo.Status == "Ready" {
			readyNodeCount++
		}
	}

	// ë…¸ë“œê°€ ì—†ê±°ë‚˜ Ready ë…¸ë“œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ failure
	if len(nodes.Items) == 0 || readyNodeCount == 0 {
		info.Status = "failure"
		log.Printf("[%s] Cluster status: FAILURE (Ready nodes: %d/%d)", name, readyNodeCount, len(nodes.Items))
	} else {
		info.Status = "ready"
		log.Printf("[%s] Cluster status: READY (Ready nodes: %d/%d)", name, readyNodeCount, len(nodes.Items))
	}

	// Pod ëª©ë¡ ì¡°íšŒ (ëª¨ë“  ë„¤ì„ìŠ¤í˜ì´ìŠ¤)
	pods, err := clientset.CoreV1().Pods("").List(ctx, metav1.ListOptions{
		LabelSelector: "app=pf-dashboard", // ëŒ€ì‹œë³´ë“œ ì•± ë¼ë²¨
	})
	if err != nil {
		log.Printf("Failed to list pods in %s: %v", contextName, err)
		// Pod ì¡°íšŒ ì‹¤íŒ¨í•´ë„ ë…¸ë“œ ì •ë³´ëŠ” ë³´ì—¬ì£¼ê¸°
	} else {
		// Running ìƒíƒœì˜ Pod ê°œìˆ˜
		runningPods := 0
		for _, pod := range pods.Items {
			podInfo := mcm.extractPodInfo(&pod)
			info.PodList = append(info.PodList, podInfo)

			if pod.Status.Phase == corev1.PodRunning {
				runningPods++
			}
		}

		info.Pods = runningPods
		info.Sessions = runningPods * 2500 // Podë‹¹ ì•½ 2500 ì„¸ì…˜ ê°€ì •
	}

	return info
}

// extractNodeInfo ë…¸ë“œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
func (mcm *MultiClusterMonitor) extractNodeInfo(node *corev1.Node) NodeInfo {
	nodeInfo := NodeInfo{
		Name: node.Name,
	}

	// Status
	for _, condition := range node.Status.Conditions {
		if condition.Type == corev1.NodeReady {
			if condition.Status == corev1.ConditionTrue {
				nodeInfo.Status = "Ready"
			} else {
				nodeInfo.Status = "NotReady"
			}
			break
		}
	}

	// Roles
	roles := []string{}
	for label := range node.Labels {
		if strings.HasPrefix(label, "node-role.kubernetes.io/") {
			role := strings.TrimPrefix(label, "node-role.kubernetes.io/")
			if role != "" {
				roles = append(roles, role)
			}
		}
	}
	if len(roles) == 0 {
		nodeInfo.Roles = "<none>"
	} else {
		nodeInfo.Roles = strings.Join(roles, ",")
	}

	// Age
	age := time.Since(node.CreationTimestamp.Time)
	nodeInfo.Age = formatDuration(age)

	// Version
	nodeInfo.Version = node.Status.NodeInfo.KubeletVersion

	// Internal IP
	for _, addr := range node.Status.Addresses {
		if addr.Type == corev1.NodeInternalIP {
			nodeInfo.InternalIP = addr.Address
			break
		}
	}

	// OS Image
	nodeInfo.OSImage = node.Status.NodeInfo.OSImage

	// Kernel Version
	nodeInfo.KernelVersion = node.Status.NodeInfo.KernelVersion

	// Container Runtime
	nodeInfo.ContainerRuntime = node.Status.NodeInfo.ContainerRuntimeVersion

	return nodeInfo
}

// extractPodInfo Pod ìƒì„¸ ì •ë³´ ì¶”ì¶œ
func (mcm *MultiClusterMonitor) extractPodInfo(pod *corev1.Pod) PodInfo {
	podInfo := PodInfo{
		Name:   pod.Name,
		Status: string(pod.Status.Phase),
		IP:     pod.Status.PodIP,
		Node:   pod.Spec.NodeName,
	}

	// Ready (ready containers / total containers)
	totalContainers := len(pod.Spec.Containers)
	readyContainers := 0
	for _, containerStatus := range pod.Status.ContainerStatuses {
		if containerStatus.Ready {
			readyContainers++
		}
	}
	podInfo.Ready = fmt.Sprintf("%d/%d", readyContainers, totalContainers)

	// Restarts
	var totalRestarts int32 = 0
	for _, containerStatus := range pod.Status.ContainerStatuses {
		totalRestarts += containerStatus.RestartCount
	}
	podInfo.Restarts = totalRestarts

	// Age
	age := time.Since(pod.CreationTimestamp.Time)
	podInfo.Age = formatDuration(age)

	return podInfo
}

// formatDuration ì‹œê°„ì„ kubectlê³¼ ìœ ì‚¬í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
func formatDuration(d time.Duration) string {
	if d < time.Minute {
		return fmt.Sprintf("%ds", int(d.Seconds()))
	} else if d < time.Hour {
		return fmt.Sprintf("%dm", int(d.Minutes()))
	} else if d < 24*time.Hour {
		return fmt.Sprintf("%dh", int(d.Hours()))
	} else {
		return fmt.Sprintf("%dd", int(d.Hours()/24))
	}
}

// Start ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ClusterMonitor ë˜í•‘)
func (mcm *MultiClusterMonitor) Start() {
	mcm.clusterMonitor.Start()
}

// GetClusters í˜„ì¬ í´ëŸ¬ìŠ¤í„° ìƒíƒœ ë°˜í™˜
func (mcm *MultiClusterMonitor) GetClusters() []ClusterInfo {
	return mcm.CheckClusters()
}

// Watch í´ëŸ¬ìŠ¤í„° ë³€ê²½ ê°ì§€
func (mcm *MultiClusterMonitor) Watch() chan []ClusterInfo {
	mcm.mu.Lock()
	defer mcm.mu.Unlock()

	watcher := make(chan []ClusterInfo, 10)
	mcm.watchers = append(mcm.watchers, watcher)
	log.Printf("[Watch] New watcher registered. Total watchers: %d", len(mcm.watchers))
	return watcher
}

// Unwatch ê°ì‹œ í•´ì œ
func (mcm *MultiClusterMonitor) Unwatch(watcher chan []ClusterInfo) {
	mcm.mu.Lock()
	defer mcm.mu.Unlock()

	for i, w := range mcm.watchers {
		if w == watcher {
			close(w)
			mcm.watchers = append(mcm.watchers[:i], mcm.watchers[i+1:]...)
			break
		}
	}
}

// NotifyWatchers ëª¨ë“  watcherì—ê²Œ ë³€ê²½ ì•Œë¦¼ (public)
func (mcm *MultiClusterMonitor) NotifyWatchers(clusters []ClusterInfo) {
	mcm.mu.RLock()
	defer mcm.mu.RUnlock()

	log.Printf("[NotifyWatchers] Notifying %d watchers with cluster data: %+v", len(mcm.watchers), clusters)

	for i, watcher := range mcm.watchers {
		select {
		case watcher <- clusters:
			log.Printf("[NotifyWatchers] Successfully sent to watcher %d", i)
		default:
			// ë²„í¼ê°€ ê°€ë“ ì°¬ ê²½ìš° ìŠ¤í‚µ
			log.Printf("[NotifyWatchers] WARNING: Watcher %d buffer full, skipping", i)
		}
	}
}

// GetServiceGraph ì„œë¹„ìŠ¤ ê·¸ë˜í”„ ì¡°íšŒ (TrafficMonitor ìœ„ì„)
func (mcm *MultiClusterMonitor) GetServiceGraph(deploymentName, namespace string) (*ServiceGraph, error) {
	return mcm.trafficMonitor.GetServiceGraph(deploymentName, namespace)
}
